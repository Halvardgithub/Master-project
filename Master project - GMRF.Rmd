---
title: "Master project"
author: "Halvard"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Simulation of a GMRF - random walks


## Random walk 1
For the random walk 1 we have a diagonal precision matrix with $2$ on the diagonal and $-1$ on the neighboring diagonals. The first and last elements on the diagonal are set to $1$, so all rows sum to $0$. $k$ is some scale constant. We also assume a zero mean, but adding a different mean is just a location shift.

```{r}
#Creating the structure matrix
n = 4
R <- diag(2, n)
R[abs(row(R) - col(R)) == 1] <- -1
R[1, 1] <- 1
R[n, n] <- 1

R

k <- 1 #A constant connected top variance
Q <- k*R
mu <- rep(0, n)
```

Eigenvalues of R:
$$
\lambda_i = 2 - 2\text{cos}(\pi (i-1)/n), \quad i = 1, ..., n
$$
Tried sampling with Algorithm 3.1, does not appear to work as expected. Maybe its missing some constraint, like the sum of $x$ equals $0$, or something with my implementation.
```{r}
# Eigenvalues
eigVecs <- eigen(Q, symmetric=TRUE) #Finds the eigenvectors

eigVals <- rep(0, n)
for(i in 2:n){
  eigVals[i] <- (2 - 2*cos(pi*(i - 1)/n))*k #Finds the eigenvalues
}

EVInv <- rev(sqrt(1/eigVals))  #Invert and square root for scaling

z <- rnorm(n-1) #Standard normal sample
y <- rep(0, n-1)
for(i in 1:(n-1)){
  y[i] <- z[i] * EVInv[i] #Computes the ys
}
x <- rep(0, n)
for(i in 1:(n-1)){
  x <- x + y[i]*eigVecs$vectors[1+i, 1:n] 
}


```

```{r}
plot(1:n, x)
```

Another approach is to mimic the algorithms from the earlier sections, however our Q is singular
```{r}
A <- eigVecs$vectors[1, 1:n]
Q_inv <- solve(Q)
det(Q)

Q_new <- Q + A%*%t(A)

L <- t(chol(Q_new)) #The lower triangle cholesky decomp.
z <- rnorm(n)
v <- solve(t(L), z)



Q_inv <- solve(Q_new)
det(Q_new)

x_adj <- x - Q_inv%*%A *solve(t(A)%*%Q_inv%*%A)*(t(A)%*%x)
```





## Basic random walk
The easiest way to simulate a random walk is through the assumption of independent normal distributed increments. We know that '
$$
x_{t+1} | x_1, ..., x_t, \sigma = x_{t+1}|x_t, \sigma \sim N(x_t, \sigma^2)
$$
Thus, all we need to do is sample from the standard normal, scale them by $\sigma$ and add them sequentially. Standard to assert that $x_0 = 0$.


```{r}
#Libraries
library(ggplot2)
library(tidyr)

library(ggpubr)
library(INLA)
```

```{r}
RW1 <- function(sigma, T){
  # sigma^2 is the variance for the transitions and T is the number of points
  x <- rep(0, T)
  z <- sigma * rnorm(T-1)
  for(j in 2:T){
    x[j] <- x[j-1] + z[j-1]
  }
  return(x)
}

#Parameters for simulation of RW1
n <- 10
T <- 100
sigma <- 1

df <- data.frame(matrix(NA, nrow = T, ncol = n))
set.seed(0)
for (i in 1:n){
  df[, i] <- RW1(sigma, T) 
}

df$t <- 1:nrow(df)  # Create a time index from 1 to n

# Reshape the dataframe to long format
df_long <- df %>% pivot_longer(cols = starts_with("X"), names_to = "variable", values_to = "value")

# Plot all lines using ggplot
RW1_plot <- ggplot(df_long, aes(x = t, y = value, color = variable)) +
  geom_line() +
  labs(title = "RW1 with N=100", x = "Time", y = "y") +
  theme(legend.position = "none")

```


## Random walk 2


$$
x_t - 2x_{t+1} + x_{t+2} \sim N(0, \sigma^2)\\
x_{t+2} \sim N(2x_{t+1} - x_t, \sigma^2) \\
x_{t+2} = 2x_{t+1} - x_t + \epsilon_t \\
\epsilon_t \sim N(0, \sigma^2)
$$

```{r}
RW2 <- function(sigma, T){
  # sigma^2 is the variance for the transitions and T is the number of points
  x <- rep(0, T)
  z <- sigma * rnorm(T-1)
  for(j in 3:T){
    x[j] <- 2*x[j-1] - x[j-2] + z[j-1]
  }
  return(x)
}

#Parameters for simulation of RW2
n <- 10
T <- 100
sigma <- 1

df2 <- data.frame(matrix(NA, nrow = T, ncol = n))
set.seed(0)
for (i in 1:n){
  df2[, i] <- RW2(sigma, T)
}

df2$t <- 1:nrow(df2)  # Create a time index from 1 to n

# Reshape the dataframe to long format
df2_long <- df2 %>% pivot_longer(cols = starts_with("X"), names_to = "variable", values_to = "value")

# Plot all lines using ggplot
RW2_plot <- ggplot(df2_long, aes(x = t, y = value, color = variable)) +
  geom_line() +
  labs(title = "RW2 with N=100", x = "Time", y = "y") +
  theme(legend.position = "none")


```


Visualize the plots.
```{r}
RW_figure <- ggarrange(RW1_plot, RW2_plot, ncol = 1)

RW_figure
```

# Simulation study
We want to conduct a small simulation study to see if the adaptive models improve the standard models in situations with shocks.First we will start with assessing the performance on non.shocked data.

## Simulation of non-shocked Gaussian data
We will simulate data with a latent temporal structured random effect as a RW1, denoted $\mathbf{x}$. The total Bayesian hierarchical model can be described as
$$
y_t | \eta_t \sim N(\eta_t, \sigma_t^2) \\
\eta_t = \mu + x_t.
$$
For the moment we assume constant $\sigma_t$ for all timepoints, and choose some fixed $\sigma_r$ for the random walk.
```{r}
#function to simulate a realization y
sim_non_shocked_gaussian_data <- function(T, mu, sigma_t, sigma_r){
  #T timepoints, mean mu, and standard deviations for time and RW1
  eta <- mu + RW1(sigma_r, T)
  y <- sapply(eta, function(r) rnorm(1, mean = r, sd = sigma_t))
  return(y)
}

sim_non_shocked_gaussian_dataframe <- function(T, mu, sigma_t, sigma_r, n, seed = 50){
  set.seed(seed)
  df <- data.frame(matrix(NA, nrow = T, ncol = n))
  for(i in 1:n){
    df[, i] <- sim_non_shocked_gaussian_data(T, mu, sigma_t, sigma_r) 
  }
  return(df)
}

test <- sim_non_shocked_gaussian_dataframe(50, 2, 0.001, 0.03, 5)

test$t <- 1:nrow(test)  # Create a time index from 1 to n

# Reshape the dataframe to long format
test_long <- test %>% pivot_longer(cols = starts_with("X"), names_to = "variable", values_to = "value")

# Plot all lines using ggplot
ggplot(test_long, aes(x = t, y = value, color = variable)) +
  geom_line() +
  labs(title = "Simulated non-shocked Gaussian data with N=50 and 5 realisations", 
       x = "Time", y = "y") + theme(legend.position = "none")
```

## Simulation of shocked Gaussian data
We now want to simulate Gaussian data where we know that there are shocks on certain timepoints. This could be modeled by adding or subtracting a slightly randomized value from the chosen points. 







## Simulation of non-shocked Poisson data
We will simulate data with a latent temporal structured random effect as a RW1. The total Bayesian hierarchical model can be described as
$$
Y|\lambda \sim Poisson(E\lambda) \\
\text{log}\lambda_t = \mu + x_t
$$
Where $\bf{x}$ $\sim RW1(\tau)$ where we fix $E, \mu$ and $\tau$ for the simulations. When fitting models we will need to assign priors to them. 

```{r}
#Simulating non-shocked data

#Parameters
E = 100
mu = 1
sigma = 0.2
T = 100 #Number of time points

#A single simulation
x <- RW1(sigma, T)
rates <- E * exp(mu + x) #rates is E*lambda or E*exp(mu + x)
y <- sapply(rates, function(r) rpois(1, r)) #samples from the Poisson

plot(1:100, x)

sim_non_shocked_data <- function(E, mu, sigma, T){
  x <- RW1(sigma, T)
  rates <- E * exp(mu + x) #rates is E*lambda or E*exp(mu + x)
  y <- sapply(rates, function(r) rpois(1, r)) #samples from the Poisson
  return(y) #The observed data
}

sim_non_shocked_dataframe <- function(E, mu, sigma, T, n, seed = 44){
  set.seed(seed)
  df <- data.frame(matrix(NA, nrow = T, ncol = n))
  for(i in 1:n){df[, i] <- sim_non_shocked_data(E, mu, sigma, T) }
  return(df)
}

test <- sim_non_shocked_dataframe(100, 4, 0.001, 100, 5)

test$t <- 1:nrow(test)  # Create a time index from 1 to n

# Reshape the dataframe to long format
test_long <- test %>% pivot_longer(cols = starts_with("X"), names_to = "variable", values_to = "value")

# Plot all lines using ggplot
ggplot(test_long, aes(x = t, y = value, color = variable)) +
  geom_line() +
  labs(title = "Simulated non-shocked data with N=100 and 5 realisations", 
       x = "Time", y = "y") + theme(legend.position = "none")
```

The code above seems to work fine, but should possibly tune some of the parameters, or maybe the plot of the ys should look insane. 

Now the next step is to fit some models with INLA, we will start with the direct smoothed model. 
## Fitting the model with INLA

```{r}
#need to input the data as a dataframe with a y column and time column
formula <- y ~ f(time, model = "rw1") #intercept is included automatically

test_data <- data.frame(matrix(c(test[, 2], 1:100, rep(E, 100)), nrow = 100, ncol = 3)) # should make the 100 more general, like T
colnames(test_data) <- c("y", "time", "E")

res <- inla(formula, E = E, family = "poisson", data = test_data)
plot(res)
summary(res)
?inla
```

Seems like it works quite well, the posterior distributions align rather well with the true parameters for mean and precision.












The following is code from a PowerPoint on Inla by Andrea Riebler, as well as some additional testing and modification by myself.

```{r}
library(INLA)
```


A simple linear regression
```{r}
# Generate data
x = sort(runif(100))
y = 1 + 2*x + rnorm(n = 100, sd = 0.1)

# Run inla
formula = y ~ 1 + x
result = inla( formula ,
  data = list(x = x , y = y) ,
  family = " gaussian ")

# Get summary
summary ( result )
plot(result)

m = result$marginals.fixed[[1]]
plot(m)
plot(inla.smarginal(m))
```

Adding a random effect
```{r}
# Generate AR (1) sequence
t = 1:100
ar = rep(0, 100)
for(i in 2:100)
  ar[i] = 0.8 *ar[i -1]+ rnorm(n = 1, sd = 0.1)

# Generate data with AR (1) component
x = runif(100)
y = 1 + 2*x + ar + rnorm(n = 100 , sd = 0.1)

# Run inla
formula = y ~ 1 + x + f(t, model="ar1")
result = inla(formula,
  data = list(x = x, y = y, t = t) ,
  family = "gaussian")

# Get summary
summary(result)
```

Prediction for an unobserved location
```{r}
# Add new location
x = c(x , 0.3)
t = c(t, 101)
y = c(y , NA)

# Re - compute
result.pred = inla(formula,
    data = list(x = x, t = t, y = y),
    family = "gaussian",
    control.predictor = list(compute = TRUE))

#m = result$marginals.linear.predictor

#m = result.pred$marginals.linear.predictor[[101]]
#round (result.pred$summary.linear.predictor[101 ,], 3)

#plot(m)
#lines(inla.smarginal(m))
```
Virker ikke helt med den marginal linear predictor greia.

Smooting binary time seires
```{r}
data ( Tokyo )
head ( Tokyo )
head ( Tokyo ,4)

# Specify linear predictor
formula = y ~ -1 + f(time , model ="rw2", cyclic = TRUE)

# Run model
result = inla(formula,
    family = "binomial",
    Ntrials = n,
    data = Tokyo)

plot(result)

#Transform to probability
result = inla ( formula,
    family = "binomial",
    Ntrials = n,
    data = Tokyo,
    control.predictor = list(compute = TRUE))
plot(result)
```

Add weights to random components
$formula = y \sim \mu + ... + f(idx, weight, model = ..., ...)$ makes the random effect term have a weight, from $\eta_i = ... + f_{idx_i}$ to $\eta_i = ... + weight_{idx_i}f_{idx_i}$. So, it adds a weight parameter.

Changing the prior
```{r}
# Old way
formula = y ~ f(idx, model = "iid", prior = "loggamma",
    param = c(1, 0.1), initial = 4, fixed = FALSE)

# New way
hyper = list(prec = list(prior = "loggamma",
    param = c(1, 0.1),
    initial = 4,
    fixed = FALSE))

formula = y ~ f(idx,model = "iid", hyper = hyper) + ...

inla.models()$latent$iid$hyper
```

Assign your own prior
```{r}
# use suitable support points x
lprec = seq( -10, 10, len =100)

# link the x and corresponding y values into a string which begins with " table :""
#With some prior function 
prior.table = INLA:::inla.paste(c("table:",cbind(lprec, prior.function(lprec))))

hyper = list(prec = list(prior = prior.table))
```

Repeated poisson count:
$$
\text{log}(\mu_{jk}) = \alpha_0 + \alpha_1 \text{log}(Base_j/4) + \alpha_2TRT_j + \alpha_3 TRT_j \text{log}(Base_j/4) + \alpha_4 Age_j + \alpha_5 V4 + Ind_j + \beta_{jk}
$$
$\alpha_i$ follows a $N(0, \tau_{\alpha})$ where $tau_{\alpha}$ is known. $Ind_j$ and $\beta_{jk}$ are the same with gamma hyperpriors on their respective precisions.

```{r}
data(Epil)
head(Epil,n =3)

formula = y ~  Base*Trt + Age + V4 +
          f(Ind, model ="iid",
          hyper = list(prec = list(prior = "loggamma",
          param = c(1, 0.01)))) +
          f(rand, model ="iid",
          hyper = list(prec = list(prior = "loggamma",
          param = c(1, 0.01))))

result = inla(formula, family ="poisson", data = Epil,
  control.fixed = list(prec.intercept = 0.001 , prec = 0.001))
```
In the control.fixed above prec.intercept is just for the intercept precision while prec is for all other fixed effects. control.predictor can compute posterior marginals for linear predictors with compute, and apply linear transformations with A. control.compute can compute measures of fit, like dic, mlik and cpo. Also many other control statements. This is done below.
```{r}
result = inla(formula, #some formula
          data = data, #some data
          control.compute = list(mlik = TRUE)) #Computes the marginal likelihood

# See result
result$mlik
result$dic$dic
result$cpo$cpo
result$cpo$pit


```
Can exchange mlik, marginal likelihood, with dic, Deviance information criteria, cpo, Conditional predictive ordinate or cpo for probability integral transform. DIC measures the trade-off between goodness of fit while trying to keep the model simple, ie. few parameters. CPO measures the fit with Bayesian hold on out cross validation for all data points. PIT:
$$
Prob(Y_i \leq y_i^{obs}|\mathbf{y}_{-i})
$$

There are several tools to extend these standard models like replicate and group, multiple likelihoods, copy, linear transformation of $\eta$ (A matrix), linear combinations, values and remote computing. Replicate simulates the random effects the requested number of times as  f(..., replicate = r [, nrep = nr ]). 
```{r}
#need to define i and r manually
formula = y ~ f(i , model ="ar1", replicate =r) + intercept -1
result = inla(formula, family = "poisson",
    data = data.frame(y, i, r, intercept))
```
Similar with groups, but these are for structured random effects, like RW1, used by  f(..., group = g [, ngroup = ng]). Looks quite strange, but for a RW2, it connects a points not only to the 2 next and 2 previous points, but also the five closest points on every other RW2.
When our data follows multiple likelihoods, we can structure the response y as a matrix or list, and apply a specific likelihood to each part of the list. Probably different hyperparameteres for each likelihood. 
```{r}
# With Y as a matrix with NA when the row is in another group, so only one 
# non NA per row, groups as columns
result = inla ( formula = Y ~ 1 + x ,
          family = c("gaussian","gaussian") ,
          data = list(Y = Y , x = x))

```

Copy allows us to use different elements of the same random effect, for example $x_i$ and $x_{i-1}$ if it doesent fit like an AR1 or RW2 or something predefined. The formula would then be: formula = y ~ f(i, model = "iid")
+ f(i.plus, copy="i") + ... when $\eta_i = u_i + u_{i-1}$. I assume the .plus shifts the sequence one to the right or something. Can also scale the copy, like f(i.plus, copy="i", hyper = list(beta=list(fixed=FALSE))). 

Can also apply a linear transformation A to $\eta$:
$$
\eta^* = A\eta \text{ with }  y_i \sim \pi (y_i | \eta_i^* , \theta)
$$
```{r} 
result = inla(formula, ... , control.predictor = list(A = A))
```
Can sometimes simplify the model and can be interchangeable with the copy feature sometimes. Can also do it as below.
```{r}
# Alternative construction
A = cbind(rep(1 , n), x)
x1 = c(1, 0) ; x2 = c(0, 1)

# Run model
result = inla ( formula = y ~ x1 + x2 - 1,
          data = list(y = y, x1 = x1, x2 = x2),
          control.predictor = list(A = A))
```

We might improve the model by using a linear combination of the latent field, like $v = Bx$ for some matrix $B$. Ideally we then use the vector $\hat{x} = [x, c]$, but this gives a much denser (less sparse) precision matrix. Can instead approximate. 
```{r}
n = 100
x = rnorm(n)
z = rnorm(n)
idx = 1:n
eta = 5 + x + z + rnorm(n)
formula = y ~ 1 + x + z + f(idx , model ="iid")
y = rpois(n, lambda = exp(eta))

# Define linear combinations
# Get alpha _x - alpha _z
lc1 = inla.make.lincomb(x =1, z = -1)
names(lc1) = "lc1"

# Get an average over all random effects
lc2 = inla.make.lincomb(idx = rep(1/n , n))
names(lc2)= "lc2"

# Run inla
r = inla(formula , "poisson", data = data.frame (y , x , z , idx ) ,
      lincomb = c( lc1 , lc2 ), 
      control.inla = list(lincomb.derived.correlation.matrix = TRUE ))
r$summary.lincomb.derived
```

RW2 with unobserved points in the middle
```{r}
# Generate data set
y = cumsum(cumsum(rnorm(100, sd =0.01)))+ rnorm (100 , sd = 0.1)
y = y[c(1:30 , 70:100)]

# Make time vector * and * value vector
t = c (1:30 , 70:100)
v = 1:100

# Run inla
#values is the time points
result = inla(formula = y ~ f(t, model ="rw2", values =v, 
          constr = FALSE ) -1 , data = list (y = y, t = t, v = v))

plot(result)
```

Can also make INLA run on a remote server with some setup, use inla.call = "remote". Something about submit and retrieve, some sort of saving of the model?

Useful tricks, set verbose = TRUE, will show size, optimizer steps and where it failed. Can also check result$logfile. 



