---
title: "Master project"
author: "Halvard"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Simulation of a GMRF - random walks


## Random walk 1
For the random walk 1 we have a diagonal precision matrix with $2$ on the diagonal and $-1$ on the neighboring diagonals. The first and last elements on the diagonal are set to $1$, so all rows sum to $0$. $k$ is some scale constant. We also assume a zero mean, but adding a different mean is just a location shift.

```{r, eval=FALSE}
#Creating the structure matrix
n = 4
R <- diag(2, n)
R[abs(row(R) - col(R)) == 1] <- -1
R[1, 1] <- 1
R[n, n] <- 1

R

k <- 1 #A constant connected top variance
Q <- k*R
mu <- rep(0, n)
```

Eigenvalues of R:
$$
\lambda_i = 2 - 2\text{cos}(\pi (i-1)/n), \quad i = 1, ..., n
$$
Tried sampling with Algorithm 3.1, does not appear to work as expected. Maybe its missing some constraint, like the sum of $x$ equals $0$, or something with my implementation.
```{r, eval=FALSE}
# Eigenvalues
eigVecs <- eigen(Q, symmetric=TRUE) #Finds the eigenvectors

eigVals <- rep(0, n)
for(i in 2:n){
  eigVals[i] <- (2 - 2*cos(pi*(i - 1)/n))*k #Finds the eigenvalues
}

EVInv <- rev(sqrt(1/eigVals))  #Invert and square root for scaling

z <- rnorm(n-1) #Standard normal sample
y <- rep(0, n-1)
for(i in 1:(n-1)){
  y[i] <- z[i] * EVInv[i] #Computes the ys
}
x <- rep(0, n)
for(i in 1:(n-1)){
  x <- x + y[i]*eigVecs$vectors[1+i, 1:n] 
}


```

```{r, eval=FALSE}
plot(1:n, x)
```

Another approach is to mimic the algorithms from the earlier sections, however our Q is singular
```{r, eval=FALSE}
A <- eigVecs$vectors[1, 1:n]
Q_inv <- solve(Q)
det(Q)

Q_new <- Q + A%*%t(A)

L <- t(chol(Q_new)) #The lower triangle cholesky decomp.
z <- rnorm(n)
v <- solve(t(L), z)



Q_inv <- solve(Q_new)
det(Q_new)

x_adj <- x - Q_inv%*%A *solve(t(A)%*%Q_inv%*%A)*(t(A)%*%x)
```





For now only using code below this point, the above code does not work.

```{r}
#Libraries
library(ggplot2)
library(tidyr)

library(ggpubr)
library(INLA)
```

## Basic random walk
The easiest way to simulate a random walk is through the assumption of independent normal distributed increments. We know that '
$$
x_{t+1} | x_1, ..., x_t, \sigma = x_{t+1}|x_t, \sigma \sim N(x_t, \sigma^2)
$$
Thus, all we need to do is sample from the standard normal, scale them by $\sigma$ and add them sequentially. Standard to assert that $x_0 = 0$.


Defining a basic plotting function that will come in handy

```{r}
plot_realizations <- function(df, title, xlabel = "t", ylabel = "y", legend = TRUE){
  #Need to give in a dataframe with fitting colnames, used by the legend
  df$t <- 1:nrow(df)  # Create a time index from 1 to n
  df_long <- df %>% pivot_longer(cols = -t, names_to = "variable", values_to = "value")
  
  ggplot(df_long, aes(x = t, y = value, color = variable)) +
  geom_line() +
  labs(title = title, 
       x = xlabel, y = ylabel) +
    if (legend) {theme(legend.title = element_blank())}
    else {theme(legend.position = "none")}
}
```

Now, lets define functionality for the random walk.
```{r}
RW1 <- function(sigma, N){
  # sigma^2 is the variance for the transitions and N is the number of points
  x <- rep(0, N)
  z <- sigma * rnorm(N-1)
  for(j in 2:N){
    x[j] <- x[j-1] + z[j-1]
  }
  return(x)
}

#Also making a normalized RW1, ie. it sums to zero
Norm_RW1 <- function(sigma, N){
  # sigma^2 is the variance for the transitions and N is the number of points
  x <- rep(0, N)
  z <- sigma * rnorm(N-1)
  for(j in 2:N){
    x[j] <- x[j-1] + z[j-1]
  }
  return(x -  mean(x)) #makes the mean zero
}

#Parameters for simulation of RW1
n <- 10
N <- 100
sigma <- 1

df <- data.frame(matrix(NA, nrow = N, ncol = n))
set.seed(0)
for (i in 1:n){
  df[, i] <- RW1(sigma, N) 
}

# Plot all lines using ggplot
RW1_plot <- plot_realizations(df, "RW1 with N=100", legend = FALSE)

```


## Random walk 2


$$
x_t - 2x_{t+1} + x_{t+2} \sim N(0, \sigma^2)\\
x_{t+2} \sim N(2x_{t+1} - x_t, \sigma^2) \\
x_{t+2} = 2x_{t+1} - x_t + \epsilon_t \\
\epsilon_t \sim N(0, \sigma^2)
$$

```{r}
RW2 <- function(sigma, N){
  # sigma^2 is the variance for the transitions and N is the number of points
  x <- rep(0, N)
  z <- sigma * rnorm(N-1)
  for(j in 3:N){
    x[j] <- 2*x[j-1] - x[j-2] + z[j-1]
  }
  return(x)
}

#Parameters for simulation of RW2
n <- 10
N <- 100
sigma <- 1

df2 <- data.frame(matrix(NA, nrow = N, ncol = n))
set.seed(0)
for (i in 1:n){
  df2[, i] <- RW2(sigma, N)
}

# Plot all lines using ggplot
RW2_plot <- plot_realizations(df2, "RW2 with N=100", legend = FALSE)
```


Visualize the plots.
```{r}
RW_figure <- ggarrange(RW1_plot, RW2_plot, ncol = 1)

RW_figure
```

# Simulation study
We want to conduct a small simulation study to see if the adaptive models improve the standard models in situations with shocks.First we will start with assessing the performance on non.shocked data.

## Simulation of non-shocked Gaussian data
We will simulate data with a latent temporal structured random effect as a RW1, denoted $\mathbf{x}$. The total Bayesian hierarchical model can be described as
$$
y_t | \eta_t \sim N(\eta_t, \sigma_t^2) \\
\eta_t = \mu + x_t.
$$
For the moment we assume constant $\sigma_t$ for all timepoints, and choose some fixed $\sigma_r$ for the random walk. First, lets make the general functions.

```{r}
#function to simulate a realization y
sim_non_shocked_gaussian_data <- function(N, mu, sigma_obs, sigma_rw){
  #N timepoints, mean mu, and standard deviations observations and the RW1
  eta <- mu + Norm_RW1(sigma_rw, N)
  y <- sapply(eta, function(r) rnorm(1, mean = r, sd = sigma_obs))
  return(y)
}

sim_non_shocked_gaussian_dataframe <- function(N, mu, sigma_obs, sigma_rw, n, seed = 50){
  set.seed(seed)
  df <- data.frame(matrix(NA, nrow = N, ncol = n))
  for(i in 1:n){
    df[, i] <- sim_non_shocked_gaussian_data(N, mu, sigma_obs, sigma_rw) 
  }
  return(df)
}

#The dataframe for all non-shocked Gaussian data
N <- 50
n <- 100
sigma_obs <- 0.001
sigma_rw <- 0.03
mu <- 2
NSG_dataframe <- sim_non_shocked_gaussian_dataframe(N, mu, sigma_obs, sigma_rw, n)

#Visualizing some simulated data
plot_realizations(NSG_dataframe[, 1:5], "Simulated non-shocked Gaussian data with N=50 and 5 realisations", legend = FALSE)
```

## Model evaluation of the the direct model for non-shocked Gaussian data
For our actual simulation study we are interested in $N=50$ timepoints and 100 realizations. We will then fit a direct smooth model and an adaptive model to each realization and compare some model criteria, like DIC etc. 

### Root mean square eroor
A common model criteria is the root mean square error, or RMSE. We define a function to calculate this below.

```{r}
RMSE <- function(data, preds){
  return(sqrt( sum((data - preds)**2) / length(data))) #definition of RMSE
}
```

### Average proper logarithmic scoring rule(LS)
From a paper by Gneiting and Raftery (2007). 
$$
\text{LS}(p, \omega) = \text{log}p(\omega)
$$
where $p$ is the predicted distribution of a point, which we get from INLA, and $\omega$ is the observed value, which is the specific datapoint. We then take the average of the result for all the data points which unique prediction densities.

it does not work yet
```{r}
LS <- function(res, data){
  #res is an inla object from calling a model on data, a vector of datapoints
  log_sum <- 0
  for (i in 1:length(data)){
    p <- (res$...something)(data[i]) # need some way to get the denisty of preds
    log_sum <- log_sum + log(p)
  }
  return(log_sum / length(data)) #returns the average
}
```


### The direct smooth model with INLA
We fill fit the simple model in INLA with a latent layer as
$$
\eta_t = \mu + x_t
$$
where $x_t$ is a RW1 with some precision $\tau$ with a default prior. We use a Gaussian likelihood in the observation layer.

```{r}
#Data preperation
NSG_data <- data.frame(matrix(c(NSG_dataframe[, 1], 1:N), nrow = N, ncol = 2)) 
colnames(NSG_data) <- c("y", "time") #makes the colnames match the formula

#The INLA model
formula <- y ~ f(time, model = "rw1") #intercept is included automatically
res <- inla(formula, family = "gaussian", data = NSG_data, 
            control.compute = list(dic = TRUE, cpo = TRUE))

#For plotting the data and the predicted values
preds <- res$summary.fitted.values$mean
plot_df <- data.frame(matrix(c(NSG_dataframe[, 1], preds), ncol = 2))
colnames(plot_df) <- c("sim_data", "preds") #for legends in the plot

plot_realizations(plot_df, "Simulated data and predicted values for non-shocked data")
```

We see that the model predictions align well with the data it is fit on.

```{r}
Model_eval_NSG <- data.frame(matrix(NA, nrow = n, ncol = 2)) # need to increase 2 later
colnames(Model_eval_NSG) <- c("DIC_DS", "RMSE")

NSG_dataframe$t <- 1:N #adds the timecolumn

formula <- y ~ f(time, model = "rw1") #intercept is included automatically
for(i in 1:n){#iterate over each simulated realization
  test_data <- NSG_dataframe[, c(i, n + 1)] #gets the i-th realization and time
  colnames(test_data) <- c("y", "time") #makes the colnames match the formula
  res_DS <- inla(formula, family = "gaussian", data = test_data, 
            control.compute = list(dic = TRUE, cpo = TRUE))
  preds <- res_DS$summary.fitted.values$mean
  Model_eval_NSG[i, ] <- c(res_DS$dic$dic, RMSE(NSG_dataframe[, i], preds))
  #can get CPO by res_DS$cpo$cpo
}

#Model_eval_NSG

#plot(res)
#summary(res)
#res$dic$dic
```






## Simulation of shocked Gaussian data
We now want to simulate Gaussian data where we know that there are shocks on certain timepoints. This could be modeled by adding or subtracting a slightly randomized value from the chosen points. Lets say we want a shock from $t=20$ to $t=30$, which could be done by adding a $s_t \overset{iid} \sim N(0.4, 0.3)$ for instance.

```{r}
#Parameters and constants
N <- 50
n <- 100
sigma_obs <- 0.001
sigma_rw <- 0.03
mu <- 2

#A comparison of non-shocked and shocked simulated data
NSG_data <- sim_non_shocked_gaussian_data(N, mu, sigma_obs, sigma_rw)
offset <- c(rep(0, 19),rnorm(11, 0.7, 0.3), rep(0, 20))
SG_data <- NSG_data + offset

example_dataframe <- data.frame(matrix(c(NSG_data, SG_data), nrow = N, ncol = 2))
colnames(example_dataframe) <- c("NS", "S") #Non-shocked and shocked

plot_realizations(example_dataframe, "Comparison of shocked and non-shocked simulated data", "t", "y")

#Non-shocked data and inlas prediction for the smooth direct model
formula <- y ~ f(time, model = "rw1") #intercept is included automatically

test_data <- data.frame(matrix(c(NSG_data, 1:N), nrow = N, ncol = 2)) 
colnames(test_data) <- c("y", "time") #makes the colnames match the formula
res_NS <- inla(formula, family = "gaussian", data = test_data)
preds_NS <- res_NS$summary.fitted.values$mean

plot_df <- data.frame(matrix(c(NSG_data, preds_NS), ncol = 2))
colnames(plot_df) <- c("sim_data", "preds")

plot_realizations(plot_df, "Simulated data and predicted values for non-shocked data", "t", "y")

#Shocked data and inlas prediction for the smooth direct model
test_data <- data.frame(matrix(c(SG_data, 1:N), nrow = N, ncol = 2)) 
colnames(test_data) <- c("y", "time") #makes the colnames match the formula
res_S <- inla(formula, family = "gaussian", data = test_data)
preds_S <- res_S$summary.fitted.values$mean

plot_df <- data.frame(matrix(c(SG_data, preds_S), ncol = 2))
colnames(plot_df) <- c("sim_data", "preds")

plot_realizations(plot_df, "Simulated data and predicted values for shocked data", "t", "y")
```

Lets make some general functions for simulating shocked Gaussian data.

```{r}
#function to simulate a shocked realization y
sim_shocked_gaussian_data <- function(N, mu, sigma_obs, sigma_rw, t_start = 20, t_end = 30, mu_offset = 0.7, sigma_offset = 0.3){
  #N timepoints, mean mu, and standard deviations observations and the RW1
  eta <- mu + Norm_RW1(sigma_rw, N)
  y <- sapply(eta, function(r) rnorm(1, mean = r, sd = sigma_obs))
  offset <- c(rep(0, t_start - 1),rnorm(t_end - t_start + 1, mu_offset, sigma_offset), rep(0, N - t_end))
  y_offset <- y + offset
  return(y_offset)
}

sim_shocked_gaussian_dataframe <- function(N, mu, sigma_obs, sigma_rw, t_start = 20, t_end = 30, mu_offset = 0.7, sigma_offset = 0.3, n, seed = 50){
  set.seed(seed)
  df <- data.frame(matrix(NA, nrow = N, ncol = n))
  for(i in 1:n){
    df[, i] <- sim_shocked_gaussian_data(N, mu, sigma_obs, sigma_rw, t_start, t_end, mu_offset, sigma_offset) 
  }
  return(df)
}

#Shocked Gaussian dataframe
SG_dataframe <- sim_shocked_gaussian_dataframe(N, mu, sigma_obs, sigma_rw, n=n)

#Visualizing some simulated data
plot_realizations(SG_dataframe[, 1:5], "Simulated shocked Gaussian data with N=50 and 5 realisations", legend = FALSE)
```



## Implementing the adaptive random walk in the latent model



Sprøsmål:


To do:
Make use of the plot function everywhere - done
Implement adaptive RW, maybe look at github og wakefield et al
Choose model criteria - somewhat done
Write about INLA
Use set.seed() to make results reproducible






## Simulation of non-shocked Poisson data
We will simulate data with a latent temporal structured random effect as a RW1. The total Bayesian hierarchical model can be described as
$$
Y|\lambda \sim Poisson(E\lambda) \\
\text{log}\lambda_t = \mu + x_t
$$
Where $\bf{x}$ $\sim RW1(\tau)$ where we fix $E, \mu$ and $\tau$ for the simulations. When fitting models we will need to assign priors to them. 

```{r}
#Simulating non-shocked data

#Parameters
E = 100
mu = 1
sigma = 0.2
T = 100 #Number of time points

#A single simulation
x <- RW1(sigma, T)
rates <- E * exp(mu + x) #rates is E*lambda or E*exp(mu + x)
y <- sapply(rates, function(r) rpois(1, r)) #samples from the Poisson

plot(1:100, x)

sim_non_shocked_data <- function(E, mu, sigma, T){
  x <- RW1(sigma, T)
  rates <- E * exp(mu + x) #rates is E*lambda or E*exp(mu + x)
  y <- sapply(rates, function(r) rpois(1, r)) #samples from the Poisson
  return(y) #The observed data
}

sim_non_shocked_dataframe <- function(E, mu, sigma, T, n, seed = 44){
  set.seed(seed)
  df <- data.frame(matrix(NA, nrow = T, ncol = n))
  for(i in 1:n){df[, i] <- sim_non_shocked_data(E, mu, sigma, T) }
  return(df)
}

test <- sim_non_shocked_dataframe(100, 4, 0.001, 100, 5)

test$t <- 1:nrow(test)  # Create a time index from 1 to n

# Reshape the dataframe to long format
test_long <- test %>% pivot_longer(cols = starts_with("X"), names_to = "variable", values_to = "value")

# Plot all lines using ggplot
ggplot(test_long, aes(x = t, y = value, color = variable)) +
  geom_line() +
  labs(title = "Simulated non-shocked data with N=100 and 5 realisations", 
       x = "Time", y = "y") + theme(legend.position = "none")
```

The code above seems to work fine, but should possibly tune some of the parameters, or maybe the plot of the ys should look insane. 

Now the next step is to fit some models with INLA, we will start with the direct smoothed model. 
## Fitting the model with INLA

```{r}
#need to input the data as a dataframe with a y column and time column
formula <- y ~ f(time, model = "rw1") #intercept is included automatically

test_data <- data.frame(matrix(c(test[, 2], 1:100, rep(E, 100)), nrow = 100, ncol = 3)) # should make the 100 more general, like T
colnames(test_data) <- c("y", "time", "E")

res <- inla(formula, E = E, family = "poisson", data = test_data)
plot(res)
summary(res)
?inla
```

Seems like it works quite well, the posterior distributions align rather well with the true parameters for mean and precision.












The following is code from a PowerPoint on Inla by Andrea Riebler, as well as some additional testing and modification by myself.

```{r}
library(INLA)
```


A simple linear regression
```{r}
# Generate data
x = sort(runif(100))
y = 1 + 2*x + rnorm(n = 100, sd = 0.1)

# Run inla
formula = y ~ 1 + x
result = inla( formula ,
  data = list(x = x , y = y) ,
  family = " gaussian ")

# Get summary
summary ( result )
plot(result)

m = result$marginals.fixed[[1]]
plot(m)
plot(inla.smarginal(m))
```

Adding a random effect
```{r}
# Generate AR (1) sequence
t = 1:100
ar = rep(0, 100)
for(i in 2:100)
  ar[i] = 0.8 *ar[i -1]+ rnorm(n = 1, sd = 0.1)

# Generate data with AR (1) component
x = runif(100)
y = 1 + 2*x + ar + rnorm(n = 100 , sd = 0.1)

# Run inla
formula = y ~ 1 + x + f(t, model="ar1")
result = inla(formula,
  data = list(x = x, y = y, t = t) ,
  family = "gaussian")

# Get summary
summary(result)
```

Prediction for an unobserved location
```{r}
# Add new location
x = c(x , 0.3)
t = c(t, 101)
y = c(y , NA)

# Re - compute
result.pred = inla(formula,
    data = list(x = x, t = t, y = y),
    family = "gaussian",
    control.predictor = list(compute = TRUE))

#m = result$marginals.linear.predictor

#m = result.pred$marginals.linear.predictor[[101]]
#round (result.pred$summary.linear.predictor[101 ,], 3)

#plot(m)
#lines(inla.smarginal(m))
```
Virker ikke helt med den marginal linear predictor greia.

Smooting binary time seires
```{r}
data ( Tokyo )
head ( Tokyo )
head ( Tokyo ,4)

# Specify linear predictor
formula = y ~ -1 + f(time , model ="rw2", cyclic = TRUE)

# Run model
result = inla(formula,
    family = "binomial",
    Ntrials = n,
    data = Tokyo)

plot(result)

#Transform to probability
result = inla ( formula,
    family = "binomial",
    Ntrials = n,
    data = Tokyo,
    control.predictor = list(compute = TRUE))
plot(result)
```

Add weights to random components
$formula = y \sim \mu + ... + f(idx, weight, model = ..., ...)$ makes the random effect term have a weight, from $\eta_i = ... + f_{idx_i}$ to $\eta_i = ... + weight_{idx_i}f_{idx_i}$. So, it adds a weight parameter.

Changing the prior
```{r}
# Old way
formula = y ~ f(idx, model = "iid", prior = "loggamma",
    param = c(1, 0.1), initial = 4, fixed = FALSE)

# New way
hyper = list(prec = list(prior = "loggamma",
    param = c(1, 0.1),
    initial = 4,
    fixed = FALSE))

formula = y ~ f(idx,model = "iid", hyper = hyper) + ...

inla.models()$latent$iid$hyper
```

Assign your own prior
```{r}
# use suitable support points x
lprec = seq( -10, 10, len =100)

# link the x and corresponding y values into a string which begins with " table :""
#With some prior function 
prior.table = INLA:::inla.paste(c("table:",cbind(lprec, prior.function(lprec))))

hyper = list(prec = list(prior = prior.table))
```

Repeated poisson count:
$$
\text{log}(\mu_{jk}) = \alpha_0 + \alpha_1 \text{log}(Base_j/4) + \alpha_2TRT_j + \alpha_3 TRT_j \text{log}(Base_j/4) + \alpha_4 Age_j + \alpha_5 V4 + Ind_j + \beta_{jk}
$$
$\alpha_i$ follows a $N(0, \tau_{\alpha})$ where $tau_{\alpha}$ is known. $Ind_j$ and $\beta_{jk}$ are the same with gamma hyperpriors on their respective precisions.

```{r}
data(Epil)
head(Epil,n =3)

formula = y ~  Base*Trt + Age + V4 +
          f(Ind, model ="iid",
          hyper = list(prec = list(prior = "loggamma",
          param = c(1, 0.01)))) +
          f(rand, model ="iid",
          hyper = list(prec = list(prior = "loggamma",
          param = c(1, 0.01))))

result = inla(formula, family ="poisson", data = Epil,
  control.fixed = list(prec.intercept = 0.001 , prec = 0.001))
```
In the control.fixed above prec.intercept is just for the intercept precision while prec is for all other fixed effects. control.predictor can compute posterior marginals for linear predictors with compute, and apply linear transformations with A. control.compute can compute measures of fit, like dic, mlik and cpo. Also many other control statements. This is done below.
```{r}
result = inla(formula, #some formula
          data = data, #some data
          control.compute = list(mlik = TRUE)) #Computes the marginal likelihood

# See result
result$mlik
result$dic$dic
result$cpo$cpo
result$cpo$pit


```
Can exchange mlik, marginal likelihood, with dic, Deviance information criteria, cpo, Conditional predictive ordinate or cpo for probability integral transform. DIC measures the trade-off between goodness of fit while trying to keep the model simple, ie. few parameters. CPO measures the fit with Bayesian hold on out cross validation for all data points. PIT:
$$
Prob(Y_i \leq y_i^{obs}|\mathbf{y}_{-i})
$$

There are several tools to extend these standard models like replicate and group, multiple likelihoods, copy, linear transformation of $\eta$ (A matrix), linear combinations, values and remote computing. Replicate simulates the random effects the requested number of times as  f(..., replicate = r [, nrep = nr ]). 
```{r}
#need to define i and r manually
formula = y ~ f(i , model ="ar1", replicate =r) + intercept -1
result = inla(formula, family = "poisson",
    data = data.frame(y, i, r, intercept))
```
Similar with groups, but these are for structured random effects, like RW1, used by  f(..., group = g [, ngroup = ng]). Looks quite strange, but for a RW2, it connects a points not only to the 2 next and 2 previous points, but also the five closest points on every other RW2.
When our data follows multiple likelihoods, we can structure the response y as a matrix or list, and apply a specific likelihood to each part of the list. Probably different hyperparameteres for each likelihood. 
```{r}
# With Y as a matrix with NA when the row is in another group, so only one 
# non NA per row, groups as columns
result = inla ( formula = Y ~ 1 + x ,
          family = c("gaussian","gaussian") ,
          data = list(Y = Y , x = x))

```

Copy allows us to use different elements of the same random effect, for example $x_i$ and $x_{i-1}$ if it doesent fit like an AR1 or RW2 or something predefined. The formula would then be: formula = y ~ f(i, model = "iid")
+ f(i.plus, copy="i") + ... when $\eta_i = u_i + u_{i-1}$. I assume the .plus shifts the sequence one to the right or something. Can also scale the copy, like f(i.plus, copy="i", hyper = list(beta=list(fixed=FALSE))). 

Can also apply a linear transformation A to $\eta$:
$$
\eta^* = A\eta \text{ with }  y_i \sim \pi (y_i | \eta_i^* , \theta)
$$
```{r} 
result = inla(formula, ... , control.predictor = list(A = A))
```
Can sometimes simplify the model and can be interchangeable with the copy feature sometimes. Can also do it as below.
```{r}
# Alternative construction
A = cbind(rep(1 , n), x)
x1 = c(1, 0) ; x2 = c(0, 1)

# Run model
result = inla ( formula = y ~ x1 + x2 - 1,
          data = list(y = y, x1 = x1, x2 = x2),
          control.predictor = list(A = A))
```

We might improve the model by using a linear combination of the latent field, like $v = Bx$ for some matrix $B$. Ideally we then use the vector $\hat{x} = [x, c]$, but this gives a much denser (less sparse) precision matrix. Can instead approximate. 
```{r}
n = 100
x = rnorm(n)
z = rnorm(n)
idx = 1:n
eta = 5 + x + z + rnorm(n)
formula = y ~ 1 + x + z + f(idx , model ="iid")
y = rpois(n, lambda = exp(eta))

# Define linear combinations
# Get alpha _x - alpha _z
lc1 = inla.make.lincomb(x =1, z = -1)
names(lc1) = "lc1"

# Get an average over all random effects
lc2 = inla.make.lincomb(idx = rep(1/n , n))
names(lc2)= "lc2"

# Run inla
r = inla(formula , "poisson", data = data.frame (y , x , z , idx ) ,
      lincomb = c( lc1 , lc2 ), 
      control.inla = list(lincomb.derived.correlation.matrix = TRUE ))
r$summary.lincomb.derived
```

RW2 with unobserved points in the middle
```{r}
# Generate data set
y = cumsum(cumsum(rnorm(100, sd =0.01)))+ rnorm (100 , sd = 0.1)
y = y[c(1:30 , 70:100)]

# Make time vector * and * value vector
t = c (1:30 , 70:100)
v = 1:100

# Run inla
#values is the time points
result = inla(formula = y ~ f(t, model ="rw2", values =v, 
          constr = FALSE ) -1 , data = list (y = y, t = t, v = v))

plot(result)
```

Can also make INLA run on a remote server with some setup, use inla.call = "remote". Something about submit and retrieve, some sort of saving of the model?

Useful tricks, set verbose = TRUE, will show size, optimizer steps and where it failed. Can also check result$logfile. 



